{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-_xdreZan0hU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "CIFAR-10"
      ]
    },
    {
      "metadata": {
        "id": "i0bzgT-hqkJp",
        "colab_type": "code",
        "outputId": "ea9defd3-5a93-4510-ef42-30c4d5812012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4942
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        " \n",
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    if epoch > 100:\n",
        "        lrate = 0.0003\n",
        "    return lrate\n",
        " \n",
        "#Loading data\n",
        "(xTrain, yTrain), (xTest, yTest) = cifar10.load_data()\n",
        "xTrain = xTrain.astype('float32')\n",
        "xTest = xTest.astype('float32')\n",
        "classes = 10\n",
        "yTrain = np_utils.to_categorical(yTrain,classes)\n",
        "yTest = np_utils.to_categorical(yTest,classes)\n",
        "\n",
        "mean = np.mean(xTrain,axis=(0,1,2,3))\n",
        "std = np.std(xTrain,axis=(0,1,2,3))\n",
        "xTrain = (xTrain-mean)/(std+1e-7)\n",
        "xTest = (xTest-mean)/(std+1e-7)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.0001), input_shape=xTrain.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(0.0001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(classes, activation='softmax'))\n",
        " \n",
        "model.summary()\n",
        " \n",
        "data = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "\n",
        "data.fit(xTrain)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.rmsprop(lr=0.001,decay=1e-6), metrics=['accuracy'])\n",
        "results = model.fit_generator(data.flow(xTrain, yTrain, batch_size=64),\\\n",
        "                    steps_per_epoch=xTrain.shape[0] // 64,epochs=110,\\\n",
        "                    verbose=1,validation_data=(xTest,yTest),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        " \n",
        "#testing\n",
        "final = model.evaluate(xTest, yTest, batch_size=128, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 26s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/110\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 1.8861 - acc: 0.4395 - val_loss: 1.6275 - val_acc: 0.5254\n",
            "Epoch 2/110\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 1.3486 - acc: 0.6036 - val_loss: 1.0951 - val_acc: 0.6705\n",
            "Epoch 3/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 1.2352 - acc: 0.6515 - val_loss: 0.9367 - val_acc: 0.7096\n",
            "Epoch 4/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 1.2282 - acc: 0.6734 - val_loss: 0.9620 - val_acc: 0.7219\n",
            "Epoch 5/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 1.1484 - acc: 0.6894 - val_loss: 0.9455 - val_acc: 0.7401\n",
            "Epoch 6/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.0513 - acc: 0.7140 - val_loss: 0.8936 - val_acc: 0.7486\n",
            "Epoch 7/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 1.0167 - acc: 0.7302 - val_loss: 0.8793 - val_acc: 0.7605\n",
            "Epoch 8/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 1.0331 - acc: 0.7348 - val_loss: 0.9388 - val_acc: 0.7321\n",
            "Epoch 9/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.9820 - acc: 0.7455 - val_loss: 0.9189 - val_acc: 0.7708\n",
            "Epoch 10/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.9975 - acc: 0.7505 - val_loss: 0.9191 - val_acc: 0.7502\n",
            "Epoch 11/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 1.0110 - acc: 0.7532 - val_loss: 1.2054 - val_acc: 0.7358\n",
            "Epoch 12/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.0002 - acc: 0.7524 - val_loss: 0.9352 - val_acc: 0.7671\n",
            "Epoch 13/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 1.0099 - acc: 0.7557 - val_loss: 0.7537 - val_acc: 0.7906\n",
            "Epoch 14/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.9407 - acc: 0.7659 - val_loss: 0.8161 - val_acc: 0.7959\n",
            "Epoch 15/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.9414 - acc: 0.7708 - val_loss: 0.8370 - val_acc: 0.7906\n",
            "Epoch 16/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.9340 - acc: 0.7720 - val_loss: 0.8714 - val_acc: 0.7832\n",
            "Epoch 17/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.9087 - acc: 0.7779 - val_loss: 0.7234 - val_acc: 0.8125\n",
            "Epoch 18/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.9150 - acc: 0.7815 - val_loss: 0.8136 - val_acc: 0.8018\n",
            "Epoch 19/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.8945 - acc: 0.7823 - val_loss: 0.7341 - val_acc: 0.8108\n",
            "Epoch 20/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.8933 - acc: 0.7855 - val_loss: 0.7579 - val_acc: 0.7989\n",
            "Epoch 21/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 1.0109 - acc: 0.7710 - val_loss: 0.9479 - val_acc: 0.7790\n",
            "Epoch 22/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 1.0108 - acc: 0.7727 - val_loss: 0.7921 - val_acc: 0.8092\n",
            "Epoch 23/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.8806 - acc: 0.7903 - val_loss: 0.7389 - val_acc: 0.8152\n",
            "Epoch 24/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.8700 - acc: 0.7927 - val_loss: 0.8300 - val_acc: 0.8020\n",
            "Epoch 25/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.8744 - acc: 0.7932 - val_loss: 0.8976 - val_acc: 0.7982\n",
            "Epoch 26/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.8530 - acc: 0.7962 - val_loss: 0.9330 - val_acc: 0.7719\n",
            "Epoch 27/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.8556 - acc: 0.7984 - val_loss: 0.7862 - val_acc: 0.8074\n",
            "Epoch 28/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.8403 - acc: 0.8020 - val_loss: 0.7197 - val_acc: 0.8257\n",
            "Epoch 29/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.8416 - acc: 0.8011 - val_loss: 0.7191 - val_acc: 0.8254\n",
            "Epoch 30/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.8233 - acc: 0.8044 - val_loss: 0.6851 - val_acc: 0.8426\n",
            "Epoch 31/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.8362 - acc: 0.8049 - val_loss: 0.7201 - val_acc: 0.8287\n",
            "Epoch 32/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.8117 - acc: 0.8080 - val_loss: 0.7203 - val_acc: 0.8270\n",
            "Epoch 33/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.8146 - acc: 0.8120 - val_loss: 0.7238 - val_acc: 0.8219\n",
            "Epoch 34/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.8130 - acc: 0.8111 - val_loss: 0.6670 - val_acc: 0.8349\n",
            "Epoch 35/110\n",
            "781/781 [==============================] - 25s 33ms/step - loss: 0.8145 - acc: 0.8090 - val_loss: 0.6714 - val_acc: 0.8395\n",
            "Epoch 36/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.8046 - acc: 0.8108 - val_loss: 0.7144 - val_acc: 0.8266\n",
            "Epoch 37/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.7983 - acc: 0.8140 - val_loss: 0.7391 - val_acc: 0.8268\n",
            "Epoch 38/110\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.8212 - acc: 0.8116 - val_loss: 0.8545 - val_acc: 0.7931\n",
            "Epoch 39/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.8013 - acc: 0.8133 - val_loss: 0.8943 - val_acc: 0.7888\n",
            "Epoch 40/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.7918 - acc: 0.8147 - val_loss: 0.7714 - val_acc: 0.8248\n",
            "Epoch 41/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.8000 - acc: 0.8134 - val_loss: 0.6878 - val_acc: 0.8291\n",
            "Epoch 42/110\n",
            "781/781 [==============================] - 25s 31ms/step - loss: 0.7875 - acc: 0.8147 - val_loss: 0.6970 - val_acc: 0.8267\n",
            "Epoch 43/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.7618 - acc: 0.8174 - val_loss: 0.7279 - val_acc: 0.8252\n",
            "Epoch 44/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.7285 - acc: 0.8187 - val_loss: 0.6808 - val_acc: 0.8362\n",
            "Epoch 45/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.7245 - acc: 0.8199 - val_loss: 0.7586 - val_acc: 0.8066\n",
            "Epoch 46/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.7250 - acc: 0.8196 - val_loss: 0.6577 - val_acc: 0.8411\n",
            "Epoch 47/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.7102 - acc: 0.8226 - val_loss: 0.6484 - val_acc: 0.8459\n",
            "Epoch 48/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.6963 - acc: 0.8253 - val_loss: 0.6921 - val_acc: 0.8360\n",
            "Epoch 49/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6849 - acc: 0.8268 - val_loss: 0.8029 - val_acc: 0.8013\n",
            "Epoch 50/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6894 - acc: 0.8270 - val_loss: 0.6115 - val_acc: 0.8520\n",
            "Epoch 51/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.6804 - acc: 0.8293 - val_loss: 0.6997 - val_acc: 0.8275\n",
            "Epoch 52/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.6716 - acc: 0.8310 - val_loss: 0.6975 - val_acc: 0.8282\n",
            "Epoch 53/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6716 - acc: 0.8285 - val_loss: 0.6563 - val_acc: 0.8392\n",
            "Epoch 54/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6652 - acc: 0.8300 - val_loss: 0.6685 - val_acc: 0.8361\n",
            "Epoch 55/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6551 - acc: 0.8325 - val_loss: 0.6646 - val_acc: 0.8316\n",
            "Epoch 56/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6454 - acc: 0.8352 - val_loss: 0.6094 - val_acc: 0.8472\n",
            "Epoch 57/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6480 - acc: 0.8341 - val_loss: 0.7200 - val_acc: 0.8204\n",
            "Epoch 58/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.6424 - acc: 0.8346 - val_loss: 0.6496 - val_acc: 0.8434\n",
            "Epoch 59/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6417 - acc: 0.8343 - val_loss: 0.6794 - val_acc: 0.8315\n",
            "Epoch 60/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6390 - acc: 0.8356 - val_loss: 0.6410 - val_acc: 0.8409\n",
            "Epoch 61/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.6287 - acc: 0.8385 - val_loss: 0.6903 - val_acc: 0.8293\n",
            "Epoch 62/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6299 - acc: 0.8379 - val_loss: 0.6103 - val_acc: 0.8505\n",
            "Epoch 63/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6233 - acc: 0.8383 - val_loss: 0.5837 - val_acc: 0.8565\n",
            "Epoch 64/110\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.6243 - acc: 0.8405 - val_loss: 0.6240 - val_acc: 0.8511\n",
            "Epoch 65/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6172 - acc: 0.8397 - val_loss: 0.6373 - val_acc: 0.8432\n",
            "Epoch 66/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6130 - acc: 0.8417 - val_loss: 0.5889 - val_acc: 0.8546\n",
            "Epoch 67/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6137 - acc: 0.8401 - val_loss: 0.5917 - val_acc: 0.8541\n",
            "Epoch 68/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.6110 - acc: 0.8415 - val_loss: 0.5939 - val_acc: 0.8553\n",
            "Epoch 69/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6100 - acc: 0.8421 - val_loss: 0.6611 - val_acc: 0.8345\n",
            "Epoch 70/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6112 - acc: 0.8443 - val_loss: 0.6966 - val_acc: 0.8301\n",
            "Epoch 71/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.6050 - acc: 0.8436 - val_loss: 0.6670 - val_acc: 0.8371\n",
            "Epoch 72/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.6057 - acc: 0.8441 - val_loss: 0.5764 - val_acc: 0.8610\n",
            "Epoch 73/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5944 - acc: 0.8455 - val_loss: 0.6046 - val_acc: 0.8505\n",
            "Epoch 74/110\n",
            "781/781 [==============================] - 25s 33ms/step - loss: 0.6005 - acc: 0.8444 - val_loss: 0.6014 - val_acc: 0.8454\n",
            "Epoch 75/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5972 - acc: 0.8459 - val_loss: 0.5982 - val_acc: 0.8542\n",
            "Epoch 76/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5987 - acc: 0.8459 - val_loss: 0.6984 - val_acc: 0.8180\n",
            "Epoch 77/110\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.5533 - acc: 0.8595 - val_loss: 0.5539 - val_acc: 0.8674\n",
            "Epoch 78/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5410 - acc: 0.8651 - val_loss: 0.5781 - val_acc: 0.8573\n",
            "Epoch 79/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5326 - acc: 0.8658 - val_loss: 0.5565 - val_acc: 0.8637\n",
            "Epoch 80/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5209 - acc: 0.8679 - val_loss: 0.5302 - val_acc: 0.8704\n",
            "Epoch 81/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.5203 - acc: 0.8665 - val_loss: 0.5967 - val_acc: 0.8523\n",
            "Epoch 82/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5183 - acc: 0.8675 - val_loss: 0.5882 - val_acc: 0.8567\n",
            "Epoch 83/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5129 - acc: 0.8691 - val_loss: 0.5430 - val_acc: 0.8689\n",
            "Epoch 84/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.5046 - acc: 0.8722 - val_loss: 0.5038 - val_acc: 0.8772\n",
            "Epoch 85/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5026 - acc: 0.8716 - val_loss: 0.5243 - val_acc: 0.8731\n",
            "Epoch 86/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.5018 - acc: 0.8698 - val_loss: 0.5313 - val_acc: 0.8687\n",
            "Epoch 87/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4965 - acc: 0.8718 - val_loss: 0.5376 - val_acc: 0.8647\n",
            "Epoch 88/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4904 - acc: 0.8734 - val_loss: 0.5049 - val_acc: 0.8768\n",
            "Epoch 89/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4921 - acc: 0.8737 - val_loss: 0.5083 - val_acc: 0.8750\n",
            "Epoch 90/110\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.4913 - acc: 0.8738 - val_loss: 0.5321 - val_acc: 0.8700\n",
            "Epoch 91/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4852 - acc: 0.8744 - val_loss: 0.5324 - val_acc: 0.8703\n",
            "Epoch 92/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4838 - acc: 0.8752 - val_loss: 0.5160 - val_acc: 0.8703\n",
            "Epoch 93/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4833 - acc: 0.8752 - val_loss: 0.5302 - val_acc: 0.8687\n",
            "Epoch 94/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4814 - acc: 0.8759 - val_loss: 0.5344 - val_acc: 0.8701\n",
            "Epoch 95/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4774 - acc: 0.8764 - val_loss: 0.5273 - val_acc: 0.8693\n",
            "Epoch 96/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4791 - acc: 0.8749 - val_loss: 0.5473 - val_acc: 0.8662\n",
            "Epoch 97/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4756 - acc: 0.8771 - val_loss: 0.5023 - val_acc: 0.8739\n",
            "Epoch 98/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4732 - acc: 0.8768 - val_loss: 0.5209 - val_acc: 0.8729\n",
            "Epoch 99/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4759 - acc: 0.8774 - val_loss: 0.5316 - val_acc: 0.8641\n",
            "Epoch 100/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4787 - acc: 0.8757 - val_loss: 0.5097 - val_acc: 0.8742\n",
            "Epoch 101/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4741 - acc: 0.8745 - val_loss: 0.5003 - val_acc: 0.8760\n",
            "Epoch 102/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4557 - acc: 0.8825 - val_loss: 0.5147 - val_acc: 0.8730\n",
            "Epoch 103/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4477 - acc: 0.8840 - val_loss: 0.5099 - val_acc: 0.8759\n",
            "Epoch 104/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4404 - acc: 0.8859 - val_loss: 0.4673 - val_acc: 0.8849\n",
            "Epoch 105/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4442 - acc: 0.8845 - val_loss: 0.5134 - val_acc: 0.8715\n",
            "Epoch 106/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4362 - acc: 0.8870 - val_loss: 0.5265 - val_acc: 0.8704\n",
            "Epoch 107/110\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 0.4319 - acc: 0.8880 - val_loss: 0.4845 - val_acc: 0.8804\n",
            "Epoch 108/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4302 - acc: 0.8895 - val_loss: 0.4667 - val_acc: 0.8849\n",
            "Epoch 109/110\n",
            "781/781 [==============================] - 24s 31ms/step - loss: 0.4341 - acc: 0.8861 - val_loss: 0.4638 - val_acc: 0.8879\n",
            "Epoch 110/110\n",
            "781/781 [==============================] - 26s 33ms/step - loss: 0.4315 - acc: 0.8888 - val_loss: 0.4942 - val_acc: 0.8753\n",
            "10000/10000 [==============================] - 1s 64us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "crTc2Wbqgequ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gvqsrulwFu1l",
        "colab_type": "code",
        "outputId": "2f45f808-3b1d-4cab-85f5-f8e083c89d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "print('\\nTest result: %.3f loss: %.3f' % (final[1]*100,final[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test result: 87.530 loss: 0.494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aePq4npJvfIk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "accuracy = final.history['acc']\n",
        "val_accuracy = final.history['val_acc']\n",
        "loss = final.history['loss']\n",
        "val_loss = final.history['val_loss']\n",
        "epochs = range(len(accuracy))\n",
        "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}